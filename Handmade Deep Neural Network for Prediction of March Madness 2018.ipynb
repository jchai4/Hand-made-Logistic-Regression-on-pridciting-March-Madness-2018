{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "np.warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>result</th>\n",
       "      <th>Adjde_diff</th>\n",
       "      <th>Adjoe_diff</th>\n",
       "      <th>Adjtempo_diff</th>\n",
       "      <th>Arate_diff</th>\n",
       "      <th>Blockpct_diff</th>\n",
       "      <th>De_diff</th>\n",
       "      <th>F3grate_diff</th>\n",
       "      <th>Fg2pct_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>Oppf3grate_diff</th>\n",
       "      <th>Oppfg2pct_diff</th>\n",
       "      <th>Oppfg3pct_diff</th>\n",
       "      <th>Oppftpct_diff</th>\n",
       "      <th>Oppstlrate_diff</th>\n",
       "      <th>Rpi_rating_diff</th>\n",
       "      <th>Seed_diff</th>\n",
       "      <th>Stlrate_diff</th>\n",
       "      <th>Temporate_diff</th>\n",
       "      <th>Preseason_winrate_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2009.637780</td>\n",
       "      <td>0.523856</td>\n",
       "      <td>-0.396469</td>\n",
       "      <td>0.422502</td>\n",
       "      <td>0.068815</td>\n",
       "      <td>0.062566</td>\n",
       "      <td>-0.018963</td>\n",
       "      <td>-0.268363</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>-0.027288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>-0.114511</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>-0.037367</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.368062</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>0.058518</td>\n",
       "      <td>0.054187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.588448</td>\n",
       "      <td>0.499674</td>\n",
       "      <td>6.592531</td>\n",
       "      <td>8.269199</td>\n",
       "      <td>4.404344</td>\n",
       "      <td>7.006210</td>\n",
       "      <td>4.478236</td>\n",
       "      <td>6.209499</td>\n",
       "      <td>7.317074</td>\n",
       "      <td>3.918753</td>\n",
       "      <td>...</td>\n",
       "      <td>5.135270</td>\n",
       "      <td>3.936761</td>\n",
       "      <td>3.032085</td>\n",
       "      <td>3.070816</td>\n",
       "      <td>1.631654</td>\n",
       "      <td>0.542585</td>\n",
       "      <td>7.510543</td>\n",
       "      <td>2.433162</td>\n",
       "      <td>4.742147</td>\n",
       "      <td>1.418783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2002.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-24.463900</td>\n",
       "      <td>-26.797900</td>\n",
       "      <td>-13.627600</td>\n",
       "      <td>-21.384200</td>\n",
       "      <td>-13.561300</td>\n",
       "      <td>-25.348300</td>\n",
       "      <td>-24.232300</td>\n",
       "      <td>-13.552600</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.469500</td>\n",
       "      <td>-12.588966</td>\n",
       "      <td>-7.693400</td>\n",
       "      <td>-8.822100</td>\n",
       "      <td>-5.220000</td>\n",
       "      <td>-2.110000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>-8.270000</td>\n",
       "      <td>-14.936700</td>\n",
       "      <td>-4.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.409000</td>\n",
       "      <td>-4.952650</td>\n",
       "      <td>-2.611100</td>\n",
       "      <td>-4.923950</td>\n",
       "      <td>-2.856514</td>\n",
       "      <td>-4.374000</td>\n",
       "      <td>-4.814950</td>\n",
       "      <td>-2.582400</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.286206</td>\n",
       "      <td>-2.815100</td>\n",
       "      <td>-2.071300</td>\n",
       "      <td>-2.170650</td>\n",
       "      <td>-1.150000</td>\n",
       "      <td>-0.280000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>-2.962050</td>\n",
       "      <td>-0.879685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.307900</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>-0.035500</td>\n",
       "      <td>-0.112900</td>\n",
       "      <td>-0.121200</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>-0.186900</td>\n",
       "      <td>-0.043500</td>\n",
       "      <td>-0.061300</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.707250</td>\n",
       "      <td>5.698450</td>\n",
       "      <td>2.773150</td>\n",
       "      <td>4.663050</td>\n",
       "      <td>2.866050</td>\n",
       "      <td>3.617050</td>\n",
       "      <td>5.070450</td>\n",
       "      <td>2.479950</td>\n",
       "      <td>...</td>\n",
       "      <td>3.213500</td>\n",
       "      <td>2.577150</td>\n",
       "      <td>2.202050</td>\n",
       "      <td>1.995300</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.605000</td>\n",
       "      <td>3.037600</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.124800</td>\n",
       "      <td>26.866300</td>\n",
       "      <td>19.116500</td>\n",
       "      <td>21.929200</td>\n",
       "      <td>14.367700</td>\n",
       "      <td>18.460100</td>\n",
       "      <td>26.233000</td>\n",
       "      <td>13.383607</td>\n",
       "      <td>...</td>\n",
       "      <td>15.262600</td>\n",
       "      <td>12.526100</td>\n",
       "      <td>9.290500</td>\n",
       "      <td>10.170700</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>20.400900</td>\n",
       "      <td>5.757576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Season       result   Adjde_diff   Adjoe_diff  Adjtempo_diff  \\\n",
       "count  1027.000000  1027.000000  1027.000000  1027.000000    1027.000000   \n",
       "mean   2009.637780     0.523856    -0.396469     0.422502       0.068815   \n",
       "std       4.588448     0.499674     6.592531     8.269199       4.404344   \n",
       "min    2002.000000     0.000000   -24.463900   -26.797900     -13.627600   \n",
       "25%    2006.000000     0.000000    -4.409000    -4.952650      -2.611100   \n",
       "50%    2010.000000     1.000000    -0.307900     0.438900       0.062500   \n",
       "75%    2014.000000     1.000000     3.707250     5.698450       2.773150   \n",
       "max    2017.000000     1.000000    21.124800    26.866300      19.116500   \n",
       "\n",
       "        Arate_diff  Blockpct_diff      De_diff  F3grate_diff  Fg2pct_diff  \\\n",
       "count  1027.000000    1027.000000  1027.000000   1027.000000  1027.000000   \n",
       "mean      0.062566      -0.018963    -0.268363     -0.003269    -0.027288   \n",
       "std       7.006210       4.478236     6.209499      7.317074     3.918753   \n",
       "min     -21.384200     -13.561300   -25.348300    -24.232300   -13.552600   \n",
       "25%      -4.923950      -2.856514    -4.374000     -4.814950    -2.582400   \n",
       "50%       0.182700      -0.035500    -0.112900     -0.121200     0.032900   \n",
       "75%       4.663050       2.866050     3.617050      5.070450     2.479950   \n",
       "max      21.929200      14.367700    18.460100     26.233000    13.383607   \n",
       "\n",
       "                ...            Oppf3grate_diff  Oppfg2pct_diff  \\\n",
       "count           ...                1027.000000     1027.000000   \n",
       "mean            ...                  -0.071892       -0.114511   \n",
       "std             ...                   5.135270        3.936761   \n",
       "min             ...                 -17.469500      -12.588966   \n",
       "25%             ...                  -3.286206       -2.815100   \n",
       "50%             ...                   0.004400       -0.186900   \n",
       "75%             ...                   3.213500        2.577150   \n",
       "max             ...                  15.262600       12.526100   \n",
       "\n",
       "       Oppfg3pct_diff  Oppftpct_diff  Oppstlrate_diff  Rpi_rating_diff  \\\n",
       "count     1027.000000    1027.000000      1027.000000      1027.000000   \n",
       "mean         0.032281      -0.037367        -0.012096         0.003320   \n",
       "std          3.032085       3.070816         1.631654         0.542585   \n",
       "min         -7.693400      -8.822100        -5.220000        -2.110000   \n",
       "25%         -2.071300      -2.170650        -1.150000        -0.280000   \n",
       "50%         -0.043500      -0.061300        -0.020000        -0.010000   \n",
       "75%          2.202050       1.995300         1.060000         0.260000   \n",
       "max          9.290500      10.170700         5.490000         1.950000   \n",
       "\n",
       "         Seed_diff  Stlrate_diff  Temporate_diff  Preseason_winrate_diff  \n",
       "count  1027.000000   1027.000000     1027.000000             1027.000000  \n",
       "mean     -0.368062      0.017286        0.058518                0.054187  \n",
       "std       7.510543      2.433162        4.742147                1.418783  \n",
       "min     -15.000000     -8.270000      -14.936700               -4.705882  \n",
       "25%      -7.000000     -1.575000       -2.962050               -0.879685  \n",
       "50%       0.000000     -0.090000        0.087000                0.086207  \n",
       "75%       5.000000      1.605000        3.037600                0.937500  \n",
       "max      15.000000      9.100000       20.400900                5.757576  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('C:\\\\Users\\hugep\\Documents\\March Madness-new\\March Madness 2018\\Data\\Data_generated.xlsx',sheet='Sheet1')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 23\n",
      "Train_X shape: (23, 727)\n",
      "Train_y shape: (1, 727)\n",
      "Test_x shape: (23, 300)\n",
      "Test_y shape: (1, 300)\n"
     ]
    }
   ],
   "source": [
    "New_data=data.as_matrix()\n",
    "np.random.shuffle(New_data)\n",
    "Train_Y_1= New_data[:727,4:5]\n",
    "Train_X_1= New_data[:727,5:]\n",
    "Test_Y_1= New_data[727:,4:5]\n",
    "Test_X_1= New_data[727:,5:]\n",
    "Train_Y=Train_Y_1.T\n",
    "Train_X=Train_X_1.T\n",
    "Test_Y=Test_Y_1.T\n",
    "Test_X=Test_X_1.T\n",
    "m_train = Train_X.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Train_X shape: \" + str(Train_X.shape))\n",
    "print (\"Train_y shape: \" + str(Train_Y.shape))\n",
    "print ('Test_x shape: '+str(Test_X.shape))\n",
    "print ('Test_y shape: '+str(Test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    \n",
    "    Z=np.array(np.dot(W,A)+b,dtype=np.float32)\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A,W,b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    S = 1/(1+np.exp(-Z))\n",
    "    cache= Z\n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    T = (np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    cache= Z\n",
    "    return T, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation =='sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation =='tanh':\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    \n",
    "    return A, cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward()\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> tanh]*(L-1).\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)],parameters['b'+str(l)], activation='tanh')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. \n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)],parameters['b'+str(L)], activation = 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(np.multiply(np.log(AL),Y)+np.multiply(np.log(1-AL),(1-Y)))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev,W,b=cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW=1/m*np.dot(dZ, A_prev.T)\n",
    "    db=1/m*np.sum(dZ,axis=1, keepdims=True)\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_backward(dA, activation_cache):\n",
    "    g = activation_cache*(1-activation_cache)\n",
    "    dZ = dA*g\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    g = 1-activation_cache\n",
    "    dZ = dA*g\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA,cache,activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'tanh':\n",
    "        dZ=tanh_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ=sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"tanh\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL)-np.divide((1-Y),(1-AL)))\n",
    "    \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"tanh\")\n",
    "        \n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):p\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_dims=(23,20,7,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                      \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.690963\n",
      "Cost after iteration 200: 0.690458\n",
      "Cost after iteration 300: 0.690330\n",
      "Cost after iteration 400: 0.690296\n",
      "Cost after iteration 500: 0.690287\n",
      "Cost after iteration 600: 0.690284\n",
      "Cost after iteration 700: 0.690283\n",
      "Cost after iteration 800: 0.690283\n",
      "Cost after iteration 900: 0.690283\n",
      "Cost after iteration 1000: 0.690283\n",
      "Cost after iteration 1100: 0.690283\n",
      "Cost after iteration 1200: 0.690283\n",
      "Cost after iteration 1300: 0.690283\n",
      "Cost after iteration 1400: 0.690283\n",
      "Cost after iteration 1500: 0.690283\n",
      "Cost after iteration 1600: 0.690283\n",
      "Cost after iteration 1700: 0.690283\n",
      "Cost after iteration 1800: 0.690283\n",
      "Cost after iteration 1900: 0.690283\n",
      "Cost after iteration 2000: 0.690283\n",
      "Cost after iteration 2100: 0.690283\n",
      "Cost after iteration 2200: 0.690283\n",
      "Cost after iteration 2300: 0.690283\n",
      "Cost after iteration 2400: 0.690283\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEWCAYAAAC0Q+rDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2208a47c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(Train_X, Train_Y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> tanh]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)],parameters['b'+str(l)], activation='tanh')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. \n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)],parameters['b'+str(L)], activation = 'sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(AL.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if AL[0,i].all() > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X,Y,parameters):\n",
    "    Y_prediction = predict(X,parameters)\n",
    "    acrc=format(100 - np.mean(np.abs(Y_prediction - Y)) * 100)\n",
    "    \n",
    "    return acrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 53.782668500687755\n",
      "Test accuracy 49.0\n"
     ]
    }
   ],
   "source": [
    "Prediction_Train = accuracy(Train_X, Train_Y, parameters)\n",
    "Prediction_Test = accuracy(Test_X, Test_Y, parameters)\n",
    "print('Training accuracy',Prediction_Train)\n",
    "print('Test accuracy',Prediction_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
